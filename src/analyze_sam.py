#!/usr/bin/env python3

# Name: analyze_sam.py
# Description: This is a python script is used by experiment 5 and 7 in order to
#              read and analyze the SAM file generated by ri-index locate
#              to generate a confusion matrix
#
# Date: June 14th, 2022

import os
import argparse
import math
import csv
import pysam
import numpy as np
import random

def calculate_accuracy_values(confusion_matrix, num_datasets):
    """ Calculates accuracy score given confusion matrix """
    accuracies = []
    for pivot in range(num_datasets):
        tp = confusion_matrix[pivot][pivot]
        fp = fn = tn = 0
        for row in range(num_datasets):
            for column in range(num_datasets):
                curr = confusion_matrix[row][column]
                if(column == pivot and row != pivot):
                    fp += curr
                elif(row == pivot and column != pivot):
                    fn += curr
                elif(row != pivot):
                    tn += curr
        accuracies.append([pivot,tp,tn,fp,fn])
    return accuracies

def main(args):
    # Set up confusion matrix and reference list
    confusion_matrix = [[0 for i in range(args.num_datasets)] for j in range(args.num_datasets)]

    # Calculate noise from FASTA index file
    if args.mems or args.half_mems:
        with open(args.fasta_index_file, "r") as fai_fd:
            all_lengths = [int(x.strip().split()[1]) for x in fai_fd.readlines()]
            total_length = sum(all_lengths)
            noise = math.log(total_length, 4)
        print(f"[log] determined the random matching statistic length ({noise:.3f}) using .fai file")

    # Go through alignments for each pivot individually - each pivot  
    # represents a row in the confusion matrix
    for i in range(args.num_datasets):
        read_mappings = {}
        input_entries = 0
        print("\n[log] building a dictionary of the read alignments for pivot {pivot}".format(pivot = i + 1))

        # Iterates through each alignment for this pivot to populate read mappings
        for j in range(args.num_datasets):
            curr_sam = pysam.AlignmentFile(args.sam_dir+"pivot_{pivot}_align_dataset_{dataset}.sam".format(pivot = i + 1, dataset = j + 1), "r")

            for read in curr_sam.fetch():
                query_length = int(read.query_name.split("_")[5])

                # Only uses reads above threshold
                if query_length >= args.t:
                    if args.mems: # Each MEM has weight = length - noise
                        if query_length - noise >= 0:
                            if read.query_name not in read_mappings:
                                read_mappings[read.query_name] = [query_length - noise]
                                input_entries += query_length - noise
                            read_mappings[read.query_name].append(j)

                        # Commented out: No noise subtract
                        #if read.query_name not in read_mappings:  
                        #    read_mappings[read.query_name] = [query_length]
                        #    input_entries += query_length
                        #read_mappings[read.query_name].append(j)

                    elif args.half_mems: # Each half-mem has weight = 1
                        if read.query_name not in read_mappings:  
                            read_mappings[read.query_name] = [query_length]
                            input_entries+=1
                        read_mappings[read.query_name].append(j)
            curr_sam.close()
        print("[log] For pivot {pivot_num}, we received a total weight of {num:.3f}".format(pivot_num=i, num=input_entries))

        # Determine the strategy we will use to build confusion matrix
        if args.read_dir is not None:
            print("[log] the confusion matrix will be generated at the read-level.")
        else:
            print("[log] the confusion matrix will be generated at the feature-level.") 
        
        # Populate confusion matrix and count entries
        total_entries = 0
        if args.read_dir is None: # Feature-level
            for key in read_mappings:
                mem_len = read_mappings[key][0] # Noise is already included here
                curr_set = set(read_mappings[key][1:])
                
                # Keep track of weight spread so far
                if args.mems:
                    total_entries += mem_len
                elif args.half_mems:
                    total_entries += 1
                
                # Spread the weight across the approriate columns in matrix
                for dataset in curr_set:
                    if args.mems:
                        confusion_matrix[i][dataset] += 1/len(curr_set) * mem_len
                    elif args.half_mems:
                        confusion_matrix[i][dataset] += 1/len(curr_set)
        else: # Read-level
            file_path = f"{args.read_dir[0]}pivot_{(i+1)}.fastq"
            nextFeatureinSameRead = lambda pos, curr_num, lines: (int(lines[pos].split("_")[1]) == curr_num)

            with open(file_path, "r") as in_fd:
                all_lines = in_fd.readlines()
                pos = 0; curr_read_num = 0

                # Go through all the features
                while pos < len(all_lines):
                    weights = [0 for i in range(args.num_datasets)]

                    # Read all features in one read
                    while pos < len(all_lines) and nextFeatureinSameRead(pos, curr_read_num, all_lines):
                        feature = all_lines[pos+1].strip()
                        length = len(feature)
                        weight = length - noise

                        # Verify features is in dictionary (might not be due to special cases
                        # like when spanning across sequences)
                        if all_lines[pos].strip()[1:] in read_mappings:
                            matches = set(read_mappings[all_lines[pos].strip()[1:]][1:])
                            for match in matches:
                                weights[match] += 1/len(matches) * weight

                        # Increment the position past current read ...
                        pos += 4
                    
                    # Update read num
                    curr_read_num += 1

                    # Determine which class is the prediction and update confusion matrix...
                    weights_np = np.array(weights)
                    max_indexes = np.where(weights_np == max(weights))[0]
                    class_decision = random.choice(max_indexes)

                    confusion_matrix[i][class_decision] += 1
                    total_entries += 1

        print("[log] For pivot {pivot_num}, we placed a weight of {num:.3f} across the row\n".format(pivot_num=i, num=total_entries))
     
    # Create output files
    print("[log] started to write out results to csv files")
    output_matrix = args.output_dir + "confusion_matrix.csv"
    output_values = args.output_dir + "accuracy_values.csv" 
    
    # Write confusion matrix to output csv
    with open(output_matrix,"w+") as csvfile:
        writer = csv.writer(csvfile)
        for row in confusion_matrix:
            writer.writerow(row)
    
    # Write accuracy values to output csv
    with open(output_values,"w+") as csvfile:
        writer = csv.writer(csvfile)
        for score in calculate_accuracy_values(confusion_matrix,args.num_datasets):
            writer.writerow(score)

def parse_arguments():
    """ Defines the command-line argument parser, and return arguments """
    parser = argparse.ArgumentParser(description="This script helps to analyze the SAM file from experiment 5"
                                                 "in order to form a confusion matrix.")
    parser.add_argument("-n", "--num", dest="num_datasets", required=True, help="number of datasets in this experiment", type=int)
    parser.add_argument("-s", "--sam_file", dest="sam_dir", required=True, help="path to directory with SAM files to be analyzed")
    parser.add_argument("-o", "--output_path", dest = "output_dir", required=True, help="path to directory for output matrix and accuracies")
    #parser.add_argument("-c","--cap", dest = "cap", required=True, help = "number of entries to cap in the confusion matrix", type = int)
    parser.add_argument("--half_mems", action="store_true", default=False, dest="half_mems", help="sam corresponds to half-mems")
    parser.add_argument("--mems", action="store_true", default=False, dest="mems", help="sam corresponds to mems (it can either be mems or half-mems, not both)")
    parser.add_argument("-t", "--threshold", dest = "t", required=False, default=0, help="optional threshold value for experiment 8", type = int)
    parser.add_argument("-l", "--length-text", dest = "fasta_index_file", required=False, default="", help="path to .fai file with total reference length")
    parser.add_argument("-r", "--read-dir", dest="read_dir", nargs=1, help="directory with read files for each pivot")
    args = parser.parse_args()
    return args

def check_arguments(args):
    """ Checks for invalid arguments """
    if not os.path.isdir(args.sam_dir):
        print("Error: sam directory does not exist.")
        exit(1) 
    if args.num_datasets < 1:
        print("Error: number of datasets must be greater than 0")

    if (args.half_mems and args.mems) or (not args.half_mems and not args.mems):
        print("Error: exactly one type needs to be chosen (--half-mems or --mems)")
        exit(1)
    if args.mems and args.fasta_index_file == "":
        print("Error: length of text must be specified when using MEMs")
        exit(1)
    if args.read_dir is not None and not os.path.isdir(args.read_dir[0]):
        print("Error: the provided directory for read files is not valid")
        exit(1)

if __name__ == "__main__":
    args = parse_arguments()
    check_arguments(args)
    main(args)

"""
# Code is not needed after using -m 1 in ri-align step

def build_refs_list(ref_lists_dir, num_datasets):
    # Creates a list of sets containing reference names for each dataset
    ref_list = []
    for i in range(1,num_datasets + 1):
        curr_file = ref_lists_dir+"dataset_{num}_references.txt".format(num = i)
        curr_set = set()
        with open(curr_file, "r") as input_fd:
            all_lines = [x.strip() for x in input_fd.readlines()]
            for line in all_lines:
                curr_set.add(line)
                curr_set.add("revcomp_"+line)
        ref_list.append(curr_set)
    return ref_list

def find_class_of_reference_name(ref_list, ref_name):
    # Finds the class that a particular reference name occurs in
    datasets = []
    for i, name_set in enumerate(ref_list):
        if ref_name in name_set:
            datasets.append(i+1)
    if len(datasets) != 1:
        print(f"Error: this read hits {len(datasets)} which is not expected.")
        exit(1)
    return datasets[0]
"""
